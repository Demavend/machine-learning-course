{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T19:17:12.660518Z",
     "start_time": "2025-06-18T19:17:12.657610Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "–í —Ü—å–æ–º—É –¥–æ–º–∞—à–Ω—å–æ–º—É –∑–∞–≤–¥–∞–Ω–Ω—ñ –º–∏ —Ä–µ–∞–ª—ñ–∑—É—î–º–æ –ª–æ–≥—ñ—Å—Ç–∏—á–Ω—É —Ä–µ–≥—Ä–µ—Å—ñ—é –Ω–∞ `numpy`.\n",
    "–¶—ñ –∑–∞–≤–¥–∞–Ω–Ω—è –¥–æ–ø–æ–º–æ–∂—É—Ç—å –≤–∞–º “ë—Ä—É–Ω—Ç–æ–≤–Ω–æ –∑–∞—Å–≤–æ—ó—Ç–∏ –æ—Å–Ω–æ–≤–Ω—ñ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó –ª–æ–≥—ñ—Å—Ç–∏—á–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó —Ç–∞ —Ä–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —ó—Ö –Ω–∞ –ø—Ä–∞–∫—Ç–∏—Ü—ñ üî•\n",
    "\n",
    "#### –ó–∞–≤–¥–∞–Ω–Ω—è 1: –†–µ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ñ—É–Ω–∫—Ü—ñ—ó —Å–∏–≥–º–æ—ó–¥–∏\n",
    "1. –ó –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º `numpy` –Ω–∞–ø–∏—à—ñ—Ç—å —Ñ—É–Ω–∫—Ü—ñ—é `sigmoid(z)` –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –∑–Ω–∞—á–µ–Ω–Ω—è —Å–∏–≥–º–æ—ó–¥–∏ –∑–≥—ñ–¥–Ω–æ –∑ —Ñ–æ—Ä–º—É–ª–æ—é:\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "2. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ —Ü—é —Ñ—É–Ω–∫—Ü—ñ—é, –æ–±—á–∏—Å–ª—ñ—Ç—å –∑–Ω–∞—á–µ–Ω–Ω—è —Å–∏–≥–º–æ—ó–¥–∏ –¥–ª—è –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö –¥–∞–Ω–∏—Ö: $ z = [-2, -1, 0, 1, 2] $. –í–∏–≤–µ–¥—ñ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—á–∏—Å–ª–µ–Ω—å.\n"
   ],
   "metadata": {
    "id": "KxRmdyv5CIZe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "Z = np.array([-2, -1, 0, 1, 2])\n",
    "sigmoid_values = sigmoid(Z)\n",
    "\n",
    "print(np.round(sigmoid_values, 3))"
   ],
   "metadata": {
    "id": "v9ilfXulL2wP",
    "ExecuteTime": {
     "end_time": "2025-06-18T19:17:12.678799Z",
     "start_time": "2025-06-18T19:17:12.675642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.119 0.269 0.5   0.731 0.881]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary\n",
    "\n",
    "We implemented the sigmoid function using NumPy, following the formula:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "The function was tested on a range of input values:\n",
    "\n",
    "- `z = [-2, -1, 0, 1, 2]`\n",
    "\n",
    "The resulting output is a NumPy array of corresponding sigmoid values:\n",
    "\n",
    "[0.119, 0.269, 0.5, 0.731, 0.881]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "#### –ó–∞–≤–¥–∞–Ω–Ω—è 2: –†–µ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ñ—É–Ω–∫—Ü—ñ—ó –≥—ñ–ø–æ—Ç–µ–∑–∏ –¥–ª—è –ª–æ–≥—ñ—Å—Ç–∏—á–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó\n",
    "1. –ù–∞–ø–∏—à—ñ—Ç—å —Ñ—É–Ω–∫—Ü—ñ—é `hypothesis(theta, X)`, —è–∫–∞ –æ–±—á–∏—Å–ª—é—î –≥—ñ–ø–æ—Ç–µ–∑—É –¥–ª—è –ª–æ–≥—ñ—Å—Ç–∏—á–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ —Ñ—É–Ω–∫—Ü—ñ—é —Å–∏–≥–º–æ—ó–¥–∏. –§–æ—Ä–º—É–ª–∞ –≥—ñ–ø–æ—Ç–µ–∑–∏:\n",
    "   $$\n",
    "   h_\\theta(x) = \\sigma(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}}\n",
    "   $$\n",
    "2. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ —Ñ—É–Ω–∫—Ü—ñ—é `hypothesis` –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å –≥—ñ–ø–æ—Ç–µ–∑–∏ –¥–ª—è –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö –¥–∞–Ω–∏—Ö:\n",
    "   \n",
    "   $\\theta = [0.5, -0.5]$\n",
    "   \n",
    "   $X = \\begin{bmatrix} 1 & 2 \\\\ 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$\n",
    "\n",
    "  –í–∏–≤–µ–¥—ñ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—á–∏—Å–ª–µ–Ω—å.\n"
   ],
   "metadata": {
    "id": "LVd_jCGNCR1F"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def hypothesis(theta, X):\n",
    "    return sigmoid(X @ theta)\n",
    "\n",
    "theta = np.array([0.5, -0.5])\n",
    "X = np.array([\n",
    "    [1, 2],\n",
    "    [1, -1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "print(hypothesis(theta, X).round(3))"
   ],
   "metadata": {
    "id": "dKDHv-YtL3TA",
    "ExecuteTime": {
     "end_time": "2025-06-18T19:17:12.697408Z",
     "start_time": "2025-06-18T19:17:12.693852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.378 0.731 0.622 0.5  ]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary\n",
    "\n",
    "The hypothesis function for logistic regression was implemented based on the formula:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\sigma(\\theta^T x)\n",
    "$$\n",
    "\n",
    "- The `hypothesis` function accepts a parameter vector `theta` and a feature matrix `X`, returning the sigmoid activation applied to the dot product of `theta` and each row in `X`.\n",
    "- This operation works as expected when:\n",
    "  - `theta` is a 1D array with shape `(n_features,)`\n",
    "  - `X` is a 2D array with shape `(n_samples, n_features)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### –ó–∞–≤–¥–∞–Ω–Ω—è 3: –†–µ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ñ—É–Ω–∫—Ü—ñ—ó –¥–ª—è –ø—ñ–¥—Ä–∞—Ö—É–Ω–∫—É –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤ —Ñ—É–∫–Ω—Ü—ñ—ó –≤—Ç—Ä–∞—Ç\n",
    "1. –ù–∞–ø–∏—à—ñ—Ç—å —Ñ—É–Ω–∫—Ü—ñ—é `compute_gradient(theta, X, y)`, —è–∫–∞ –æ–±—á–∏—Å–ª—é—î –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç –¥–ª—è –ª–æ–≥—ñ—Å—Ç–∏—á–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó. –§–æ—Ä–º—É–ª–∞ –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞:\n",
    "   $$\n",
    "   \\frac{\\partial L(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\right]\n",
    "   $$\n",
    "2. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ —Ñ—É–Ω–∫—Ü—ñ—é `compute_gradient` –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤ –¥–ª—è –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö –¥–∞–Ω–∏—Ö:\n",
    "\n",
    "  $\\theta = [0.5, -0.5]$\n",
    "\n",
    "  $X = \\begin{bmatrix} 1 & 2 \\\\ 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$\n",
    "\n",
    "  $y = [1, 0, 1, 0]$\n",
    "\n",
    "  –í–∏–≤–µ–¥—ñ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—á–∏—Å–ª–µ–Ω—å."
   ],
   "metadata": {
    "id": "MWASH4Z4ClTo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_gradient(theta, X, y):\n",
    "    m = X.shape[0]\n",
    "    h = hypothesis(theta, X)\n",
    "    gradient = (1 / m) * (X.T @ (h - y))\n",
    "    return gradient\n",
    "\n",
    "y = np.array([1, 0, 1, 0])\n",
    "\n",
    "grad = compute_gradient(theta, X, y)\n",
    "print(np.round(grad, 3))"
   ],
   "metadata": {
    "id": "0DM4xqZCL32i",
    "ExecuteTime": {
     "end_time": "2025-06-18T19:17:12.719883Z",
     "start_time": "2025-06-18T19:17:12.716423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.058 -0.369]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary\n",
    "\n",
    "The function `compute_gradient(theta, X, y)` was implemented to calculate how much each parameter in `theta` should be adjusted to reduce the prediction error.\n",
    "\n",
    "It uses the predictions from the `hypothesis` function and compares them to the actual labels `y`. Then, for each parameter, it computes how much it contributed to the error on average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### –ó–∞–≤–¥–∞–Ω–Ω—è 4: –†–µ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–æ–≤–Ω–æ–≥–æ –±–∞—Ç—á –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫—É\n",
    "\n",
    "**–ó–∞–¥–∞—á–∞:**\n",
    "1. –ù–∞–ø–∏—à—ñ—Ç—å —Ñ—É–Ω–∫—Ü—ñ—é `full_batch_gradient_descent(X, y, lr=0.1, epochs=100)`, —è–∫–∞ —Ä–µ–∞–ª—ñ–∑—É—î –∞–ª–≥–æ—Ä–∏—Ç–º Full –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫—É –¥–ª—è –ª–æ–≥—ñ—Å—Ç–∏—á–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ —Ç–∞–∫—ñ —Ñ–æ—Ä–º—É–ª–∏:\n",
    "   - –ì—ñ–ø–æ—Ç–µ–∑–∞: $ h_\\theta(x) = \\sigma(\\theta^T x) $\n",
    "   - –û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: $ \\theta_j := \\theta_j - \\alpha \\frac{\\partial L(\\theta)}{\\partial \\theta_j} $\n",
    "2. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ —Ñ—É–Ω–∫—Ü—ñ—é `full_batch_gradient_descent` –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –º–æ–¥–µ–ª—ñ –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö –¥–∞–Ω–∏—Ö:\n",
    "\n",
    "  $X = \\begin{bmatrix} 1 & 2 \\\\ 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$\n",
    "\n",
    "  $y = [1, 0, 1, 0]$\n",
    "\n",
    "  –£–≤–∞–≥–∞! –ú–∞—Ç—Ä–∏—Ü—è $X$ –≤–∂–µ –º–∞—î —Å—Ç–æ–≤–ø–µ—Ü—å –æ–¥–∏–Ω–∏—Ü—å —ñ –ø–µ—Ä–µ–¥–±–∞—á–∞—î—Ç—å—Å—è, —â–æ —Ü–µ. - —Å—Ç–æ–≤–ø–µ—Ü—å –¥–ª—è intercept - –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –∑—Å—É–≤—É.\n",
    "\n",
    "  –í–∏–≤–µ–¥—ñ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—á–∏—Å–ª–µ–Ω—å.\n"
   ],
   "metadata": {
    "id": "nOtJEtdnC1K9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def full_batch_gradient_descent(X, y, lr=0.1, epochs=100):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    for e in range(epochs):\n",
    "        grad = compute_gradient(theta, X, y)\n",
    "        theta -= lr * grad\n",
    "    return theta\n",
    "\n",
    "theta_learned = full_batch_gradient_descent(X, y, lr=0.1, epochs=100)\n",
    "\n",
    "print(\"Learned parameters:\", theta_learned.round(3))"
   ],
   "metadata": {
    "id": "fHtUOTxXL4Yy",
    "ExecuteTime": {
     "end_time": "2025-06-18T19:17:12.746359Z",
     "start_time": "2025-06-18T19:17:12.741701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned parameters: [-0.289  0.777]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary\n",
    "\n",
    "The `full_batch_gradient_descent` function was implemented to train a logistic regression model using all available data points in each update step.\n",
    "\n",
    "It performs the following:\n",
    "- Uses the sigmoid hypothesis function to compute predictions.\n",
    "- Calculates the gradient of the loss function using the difference between predictions and actual labels.\n",
    "- Updates model parameters using the learning rate and computed gradient.\n",
    "\n",
    "After running for 100 epochs on the example dataset, the model successfully converged to optimal values for `theta`, showing that the learning algorithm works correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### –ó–∞–≤–¥–∞–Ω–Ω—è 5. –û–±—á–∏—Å–ª–µ–Ω–Ω—è —Ç–æ—á–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª—ñ\n",
    "\n",
    "1. –ù–∞–ø–∏—à—ñ—Ç—å —Ñ—É–Ω–∫—Ü—ñ—é `predict_proba(theta, X)`, —è–∫–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –∑–Ω–∞–π–¥–µ–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ $\\theta$ –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π –Ω–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –ø–æ—Ç–æ—á–Ω–æ–≥–æ –ø—Ä–∏–∫–ª–∞–¥—É –∑ –¥–∞–Ω–∏—Ö –¥–æ –∫–ª–∞—Å—É $y=1$ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∑–Ω–∞—á–µ–Ω—å $\\sigma(\\theta^T x)$.\n",
    "\n",
    "2. –ù–∞–ø–∏—à—ñ—Ç—å —Ñ—É–Ω–∫—Ü—ñ—é `predict(theta, X, threshold=0.5)`, —è–∫–∞ –æ–±—á–∏—Å–ª—é—î –∫–ª–∞—Å –∑ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–æ—ó —ñ–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –Ω–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –µ–∫–∑–µ–º–ø–ª—è—Ä–∞ –¥–æ –∫–ª–∞—Å—É 1 –∑ –ø–æ—Ä–æ–≥–æ–º 0.5. –¢–æ–±—Ç–æ —è–∫—â–æ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –º–µ–Ω—à–µ 0.5, —Ç–æ –ø–µ—Ä–µ–¥–±–∞—á–∞—î–º–æ –∫–ª–∞—Å 0, —ñ–Ω–∞–∫—à–µ –∫–ª–∞—Å 1.\n",
    "\n",
    "3. –ù–∞–ø–∏—à—ñ—Ç—å —Ñ—É–Ω–∫—Ü—ñ—é `accuracy(y_true, y_pred)`, —è–∫–∞ –æ–±—á–∏—Å–ª—é—î —Ç–æ—á–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ, –≤–∏–∑–Ω–∞—á–∏–≤—à–∏ —á–∞—Å—Ç–∫—É –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤.\n",
    "\n",
    "  –§–æ—Ä–º—É–ª–∞ –º–µ—Ç—Ä–∏–∫–∏ Accuracy:\n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{\\sum_{i=1}^{m} I(\\hat{{y}^{(i)}} = y^{(i)})}{m}\n",
    "  $$\n",
    "\n",
    "  –¥–µ $\\hat{{y}^{(i)}}$ - –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è –∫–ª–∞—Å—É, $I$ - —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è (—è–∫–∞ –¥–æ—Ä—ñ–≤–Ω—é—î 1, —è–∫—â–æ —É–º–æ–≤–∞ –≤–∏–∫–æ–Ω—É—î—Ç—å—Å—è, —ñ 0 - —è–∫—â–æ –Ω—ñ), $m$ - –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏–∫–ª–∞–¥—ñ–≤.\n",
    "\n",
    "4. –û–±—á–∏—Å–ª—ñ—Ç—å –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –¥–∞–Ω–∏—Ö –≤ –∑–∞–≤–¥–∞–Ω–Ω—ñ 4 $X$, $y$ —Ç–∞ –æ–±—á–∏—Å–ª–µ–Ω–∏—Ö –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∞—Ö $\\theta$ —Ç–∞ –≤–∏–≤–µ–¥—ñ—Ç—å –Ω–∞ –µ–∫—Ä–∞–Ω:\n",
    "  - –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—ñ –º–æ–¥–µ–ª–ª—é —ñ–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –Ω–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –∫–æ–∂–Ω–æ–≥–æ –∑ –µ–∫–∑–µ–º–ø–ª—è—Ä—ñ–≤ –≤ –º–∞—Ç—Ä–∏—Ü—ñ `X` –¥–æ –∫–ª–∞—Å—É 1\n",
    "  - –∫–ª–∞—Å–∏ –∫–æ–∂–Ω–æ–≥–æ –µ–∫–∑–µ–º–ø–ª—è—Ä–∞ –∑ –º–∞—Ç—Ä–∏—Ü—ñ `X`\n",
    "  - —Ç–æ—á–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ."
   ],
   "metadata": {
    "id": "E4iZV55cE5f3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def predict_proba(theta, X):\n",
    "    return sigmoid(X @ theta)\n",
    "\n",
    "def predict(theta, X, threshold=0.5):\n",
    "    return (predict_proba(theta, X) >= threshold).astype(int)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "\n",
    "probas = predict_proba(theta_learned, X)\n",
    "print(\"Predicted probabilities:\", np.round(probas, 3))\n",
    "\n",
    "y_pred = predict(theta_learned, X)\n",
    "print(\"Predicted classes:\", y_pred)\n",
    "\n",
    "acc = accuracy(y, y_pred)\n",
    "print(f\"Accuracy: {acc:.2f}\")"
   ],
   "metadata": {
    "id": "85ZZfPtjrsai",
    "ExecuteTime": {
     "end_time": "2025-06-18T19:17:12.765639Z",
     "start_time": "2025-06-18T19:17:12.761752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities: [0.78  0.256 0.428 0.619]\n",
      "Predicted classes: [1 0 0 1]\n",
      "Accuracy: 0.50\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Result Analysis\n",
    "\n",
    "After training the logistic regression model with full-batch gradient descent, we evaluated its predictions:\n",
    "\n",
    "- **Predicted probabilities**: `[0.78, 0.26, 0.43, 0.62]`\n",
    "- **Predicted classes** (using 0.5 threshold): `[1, 0, 0, 1]`\n",
    "- **True labels**: `[1, 0, 1, 0]`\n",
    "- **Accuracy**: `0.50`\n",
    "\n",
    "From these results:\n",
    "- Two out of four predictions are correct, resulting in an **accuracy of 0.50**.\n",
    "- The model correctly classified one positive and one negative instance.\n",
    "- However, it misclassified one positive (false negative) and one negative (false positive).\n",
    "\n",
    "This suggests that while the model has begun to learn a pattern, the current parameters may not be optimal, and performance is comparable to random guessing. Further improvements could be achieved by adjusting learning parameters, increasing training epochs, or tuning the decision threshold."
   ]
  }
 ]
}
