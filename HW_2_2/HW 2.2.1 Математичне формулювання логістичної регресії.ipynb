{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T19:17:12.660518Z",
     "start_time": "2025-06-18T19:17:12.657610Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ð’ Ñ†ÑŒÐ¾Ð¼Ñƒ Ð´Ð¾Ð¼Ð°ÑˆÐ½ÑŒÐ¾Ð¼Ñƒ Ð·Ð°Ð²Ð´Ð°Ð½Ð½Ñ– Ð¼Ð¸ Ñ€ÐµÐ°Ð»Ñ–Ð·ÑƒÑ”Ð¼Ð¾ Ð»Ð¾Ð³Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ñƒ Ñ€ÐµÐ³Ñ€ÐµÑÑ–ÑŽ Ð½Ð° `numpy`.\n",
    "Ð¦Ñ– Ð·Ð°Ð²Ð´Ð°Ð½Ð½Ñ Ð´Ð¾Ð¿Ð¾Ð¼Ð¾Ð¶ÑƒÑ‚ÑŒ Ð²Ð°Ð¼ Ò‘Ñ€ÑƒÐ½Ñ‚Ð¾Ð²Ð½Ð¾ Ð·Ð°ÑÐ²Ð¾Ñ—Ñ‚Ð¸ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ– ÐºÐ¾Ð½Ñ†ÐµÐ¿Ñ†Ñ–Ñ— Ð»Ð¾Ð³Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ð¾Ñ— Ñ€ÐµÐ³Ñ€ÐµÑÑ–Ñ— Ñ‚Ð° Ñ€ÐµÐ°Ð»Ñ–Ð·ÑƒÐ²Ð°Ñ‚Ð¸ Ñ—Ñ… Ð½Ð° Ð¿Ñ€Ð°ÐºÑ‚Ð¸Ñ†Ñ– ðŸ”¥\n",
    "\n",
    "#### Ð—Ð°Ð²Ð´Ð°Ð½Ð½Ñ 1: Ð ÐµÐ°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ Ñ„ÑƒÐ½ÐºÑ†Ñ–Ñ— ÑÐ¸Ð³Ð¼Ð¾Ñ—Ð´Ð¸\n",
    "1. Ð— Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½ÑÐ¼ `numpy` Ð½Ð°Ð¿Ð¸ÑˆÑ–Ñ‚ÑŒ Ñ„ÑƒÐ½ÐºÑ†Ñ–ÑŽ `sigmoid(z)` Ð´Ð»Ñ Ð¾Ð±Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ ÑÐ¸Ð³Ð¼Ð¾Ñ—Ð´Ð¸ Ð·Ð³Ñ–Ð´Ð½Ð¾ Ð· Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¾ÑŽ:\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "2. Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑŽÑ‡Ð¸ Ñ†ÑŽ Ñ„ÑƒÐ½ÐºÑ†Ñ–ÑŽ, Ð¾Ð±Ñ‡Ð¸ÑÐ»Ñ–Ñ‚ÑŒ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ ÑÐ¸Ð³Ð¼Ð¾Ñ—Ð´Ð¸ Ð´Ð»Ñ Ð½Ð°ÑÑ‚ÑƒÐ¿Ð½Ð¸Ñ… Ð´Ð°Ð½Ð¸Ñ…: $ z = [-2, -1, 0, 1, 2] $. Ð’Ð¸Ð²ÐµÐ´Ñ–Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¾Ð±Ñ‡Ð¸ÑÐ»ÐµÐ½ÑŒ.\n"
   ],
   "metadata": {
    "id": "KxRmdyv5CIZe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "Z = np.array([-2, -1, 0, 1, 2])\n",
    "sigmoid_values = sigmoid(Z)\n",
    "\n",
    "print(np.round(sigmoid_values, 3))"
   ],
   "metadata": {
    "id": "v9ilfXulL2wP",
    "ExecuteTime": {
     "end_time": "2025-06-18T19:17:12.678799Z",
     "start_time": "2025-06-18T19:17:12.675642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.119 0.269 0.5   0.731 0.881]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary\n",
    "\n",
    "We implemented the sigmoid function using NumPy, following the formula:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "The function was tested on a range of input values:\n",
    "\n",
    "- `z = [-2, -1, 0, 1, 2]`\n",
    "\n",
    "The resulting output is a NumPy array of corresponding sigmoid values:\n",
    "\n",
    "[0.119, 0.269, 0.5, 0.731, 0.881]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "#### Ð—Ð°Ð²Ð´Ð°Ð½Ð½Ñ 2: Ð ÐµÐ°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ Ñ„ÑƒÐ½ÐºÑ†Ñ–Ñ— Ð³Ñ–Ð¿Ð¾Ñ‚ÐµÐ·Ð¸ Ð´Ð»Ñ Ð»Ð¾Ð³Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ð¾Ñ— Ñ€ÐµÐ³Ñ€ÐµÑÑ–Ñ—\n",
    "1. ÐÐ°Ð¿Ð¸ÑˆÑ–Ñ‚ÑŒ Ñ„ÑƒÐ½ÐºÑ†Ñ–ÑŽ `hypothesis(theta, X)`, ÑÐºÐ° Ð¾Ð±Ñ‡Ð¸ÑÐ»ÑŽÑ” Ð³Ñ–Ð¿Ð¾Ñ‚ÐµÐ·Ñƒ Ð´Ð»Ñ Ð»Ð¾Ð³Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ð¾Ñ— Ñ€ÐµÐ³Ñ€ÐµÑÑ–Ñ—, Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑŽÑ‡Ð¸ Ñ„ÑƒÐ½ÐºÑ†Ñ–ÑŽ ÑÐ¸Ð³Ð¼Ð¾Ñ—Ð´Ð¸. Ð¤Ð¾Ñ€Ð¼ÑƒÐ»Ð° Ð³Ñ–Ð¿Ð¾Ñ‚ÐµÐ·Ð¸:\n",
    "   $$\n",
    "   h_\\theta(x) = \\sigma(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}}\n",
    "   $$\n",
    "2. Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð¹Ñ‚Ðµ Ñ„ÑƒÐ½ÐºÑ†Ñ–ÑŽ `hypothesis` Ð´Ð»Ñ Ð¾Ð±Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ Ð·Ð½Ð°Ñ‡ÐµÐ½ÑŒ Ð³Ñ–Ð¿Ð¾Ñ‚ÐµÐ·Ð¸ Ð´Ð»Ñ Ð½Ð°ÑÑ‚ÑƒÐ¿Ð½Ð¸Ñ… Ð´Ð°Ð½Ð¸Ñ…:\n",
    "   \n",
    "   $\\theta = [0.5, -0.5]$\n",
    "   \n",
    "   $X = \\begin{bmatrix} 1 & 2 \\\\ 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$\n",
    "\n",
    "  Ð’Ð¸Ð²ÐµÐ´Ñ–Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¾Ð±Ñ‡Ð¸ÑÐ»ÐµÐ½ÑŒ.\n"
   ],
   "metadata": {
    "id": "LVd_jCGNCR1F"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def hypothesis(theta, X):\n",
    "    return sigmoid(X @ theta)\n",
    "\n",
    "theta = np.array([0.5, -0.5])\n",
    "X = np.array([\n",
    "    [1, 2],\n",
    "    [1, -1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "print(hypothesis(theta, X).round(3))"
   ],
   "metadata": {
    "id": "dKDHv-YtL3TA",
    "ExecuteTime": {
     "end_time": "2025-06-18T19:17:12.697408Z",
     "start_time": "2025-06-18T19:17:12.693852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.378 0.731 0.622 0.5  ]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary\n",
    "\n",
    "The hypothesis function for logistic regression was implemented based on the formula:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\sigma(\\theta^T x)\n",
    "$$\n",
    "\n",
    "- The `hypothesis` function accepts a parameter vector `theta` and a feature matrix `X`, returning the sigmoid activation applied to the dot product of `theta` and each row in `X`.\n",
    "- This operation works as expected when:\n",
    "  - `theta` is a 1D array with shape `(n_features,)`\n",
    "  - `X` is a 2D array with shape `(n_samples, n_features)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Ð—Ð°Ð²Ð´Ð°Ð½Ð½Ñ 3: Ð ÐµÐ°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ Ñ„ÑƒÐ½ÐºÑ†Ñ–Ñ— Ð´Ð»Ñ Ð¿Ñ–Ð´Ñ€Ð°Ñ…ÑƒÐ½ÐºÑƒ Ð³Ñ€Ð°Ð´Ñ–Ñ”Ð½Ñ‚Ñ–Ð² Ñ„ÑƒÐºÐ½Ñ†Ñ–Ñ— Ð²Ñ‚Ñ€Ð°Ñ‚\n",
    "1. ÐÐ°Ð¿Ð¸ÑˆÑ–Ñ‚ÑŒ Ñ„ÑƒÐ½ÐºÑ†Ñ–ÑŽ `compute_gradient(theta, X, y)`, ÑÐºÐ° Ð¾Ð±Ñ‡Ð¸ÑÐ»ÑŽÑ” Ð³Ñ€Ð°Ð´Ñ–Ñ”Ð½Ñ‚Ð¸ Ñ„ÑƒÐ½ÐºÑ†Ñ–Ñ— Ð²Ñ‚Ñ€Ð°Ñ‚ Ð´Ð»Ñ Ð»Ð¾Ð³Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ð¾Ñ— Ñ€ÐµÐ³Ñ€ÐµÑÑ–Ñ—. Ð¤Ð¾Ñ€Ð¼ÑƒÐ»Ð° Ð´Ð»Ñ Ð¾Ð±Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ Ð³Ñ€Ð°Ð´Ñ–Ñ”Ð½Ñ‚Ð°:\n",
    "   $$\n",
    "   \\frac{\\partial L(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\right]\n",
    "   $$\n",
    "2. Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð¹Ñ‚Ðµ Ñ„ÑƒÐ½ÐºÑ†Ñ–ÑŽ `compute_gradient` Ð´Ð»Ñ Ð¾Ð±Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ Ð³Ñ€Ð°Ð´Ñ–Ñ”Ð½Ñ‚Ñ–Ð² Ð´Ð»Ñ Ð½Ð°ÑÑ‚ÑƒÐ¿Ð½Ð¸Ñ… Ð´Ð°Ð½Ð¸Ñ…:\n",
    "\n",
    "  $\\theta = [0.5, -0.5]$\n",
    "\n",
    "  $X = \\begin{bmatrix} 1 & 2 \\\\ 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$\n",
    "\n",
    "  $y = [1, 0, 1, 0]$\n",
    "\n",
    "  Ð’Ð¸Ð²ÐµÐ´Ñ–Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¾Ð±Ñ‡Ð¸ÑÐ»ÐµÐ½ÑŒ."
   ],
   "metadata": {
    "id": "MWASH4Z4ClTo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_gradient(theta, X, y):\n",
    "    m = X.shape[0]\n",
    "    h = hypothesis(theta, X)\n",
    "    gradient = (1 / m) * (X.T @ (h - y))\n",
    "    return gradient\n",
    "\n",
    "y = np.array([1, 0, 1, 0])\n",
    "\n",
    "grad = compute_gradient(theta, X, y)\n",
    "print(np.round(grad, 3))"
   ],
   "metadata": {
    "id": "0DM4xqZCL32i",
    "ExecuteTime": {
     "end_time": "2025-06-18T19:17:12.719883Z",
     "start_time": "2025-06-18T19:17:12.716423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.058 -0.369]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary\n",
    "\n",
    "The function `compute_gradient(theta, X, y)` was implemented to calculate how much each parameter in `theta` should be adjusted to reduce the prediction error.\n",
    "\n",
    "It uses the predictions from the `hypothesis` function and compares them to the actual labels `y`. Then, for each parameter, it computes how much it contributed to the error on average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Ð—Ð°Ð²Ð´Ð°Ð½Ð½Ñ 4: Ð ÐµÐ°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ Ð¿Ð¾Ð²Ð½Ð¾Ð³Ð¾ Ð±Ð°Ñ‚Ñ‡ Ð³Ñ€Ð°Ð´Ñ–Ñ”Ð½Ñ‚Ð½Ð¾Ð³Ð¾ ÑÐ¿ÑƒÑÐºÑƒ\n",
    "\n",
    "**Ð—Ð°Ð´Ð°Ñ‡Ð°:**\n",
    "1. ÐÐ°Ð¿Ð¸ÑˆÑ–Ñ‚ÑŒ Ñ„ÑƒÐ½ÐºÑ†Ñ–ÑŽ `full_batch_gradient_descent(X, y, lr=0.1, epochs=100)`, ÑÐºÐ° Ñ€ÐµÐ°Ð»Ñ–Ð·ÑƒÑ” Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼ Full Ð³Ñ€Ð°Ð´Ñ–Ñ”Ð½Ñ‚Ð½Ð¾Ð³Ð¾ ÑÐ¿ÑƒÑÐºÑƒ Ð´Ð»Ñ Ð»Ð¾Ð³Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ð¾Ñ— Ñ€ÐµÐ³Ñ€ÐµÑÑ–Ñ—. Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ¹Ñ‚Ðµ Ñ‚Ð°ÐºÑ– Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸:\n",
    "   - Ð“Ñ–Ð¿Ð¾Ñ‚ÐµÐ·Ð°: $ h_\\theta(x) = \\sigma(\\theta^T x) $\n",
    "   - ÐžÐ½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ–Ð²: $ \\theta_j := \\theta_j - \\alpha \\frac{\\partial L(\\theta)}{\\partial \\theta_j} $\n",
    "2. Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð¹Ñ‚Ðµ Ñ„ÑƒÐ½ÐºÑ†Ñ–ÑŽ `full_batch_gradient_descent` Ð´Ð»Ñ Ð¾Ð±Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ–Ð² Ð¼Ð¾Ð´ÐµÐ»Ñ– Ð½Ð° Ð½Ð°ÑÑ‚ÑƒÐ¿Ð½Ð¸Ñ… Ð´Ð°Ð½Ð¸Ñ…:\n",
    "\n",
    "  $X = \\begin{bmatrix} 1 & 2 \\\\ 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$\n",
    "\n",
    "  $y = [1, 0, 1, 0]$\n",
    "\n",
    "  Ð£Ð²Ð°Ð³Ð°! ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ñ $X$ Ð²Ð¶Ðµ Ð¼Ð°Ñ” ÑÑ‚Ð¾Ð²Ð¿ÐµÑ†ÑŒ Ð¾Ð´Ð¸Ð½Ð¸Ñ†ÑŒ Ñ– Ð¿ÐµÑ€ÐµÐ´Ð±Ð°Ñ‡Ð°Ñ”Ñ‚ÑŒÑÑ, Ñ‰Ð¾ Ñ†Ðµ. - ÑÑ‚Ð¾Ð²Ð¿ÐµÑ†ÑŒ Ð´Ð»Ñ intercept - Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð° Ð·ÑÑƒÐ²Ñƒ.\n",
    "\n",
    "  Ð’Ð¸Ð²ÐµÐ´Ñ–Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð¾Ð±Ñ‡Ð¸ÑÐ»ÐµÐ½ÑŒ.\n"
   ],
   "metadata": {
    "id": "nOtJEtdnC1K9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def full_batch_gradient_descent(X, y, lr=0.1, epochs=100):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    for e in range(epochs):\n",
    "        grad = compute_gradient(theta, X, y)\n",
    "        theta -= lr * grad\n",
    "    return theta\n",
    "\n",
    "theta_learned = full_batch_gradient_descent(X, y, lr=0.1, epochs=100)\n",
    "\n",
    "print(\"Learned parameters:\", theta_learned.round(3))"
   ],
   "metadata": {
    "id": "fHtUOTxXL4Yy",
    "ExecuteTime": {
     "end_time": "2025-06-18T19:17:12.746359Z",
     "start_time": "2025-06-18T19:17:12.741701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned parameters: [-0.289  0.777]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary\n",
    "\n",
    "The `full_batch_gradient_descent` function was implemented to train a logistic regression model using all available data points in each update step.\n",
    "\n",
    "It performs the following:\n",
    "- Uses the sigmoid hypothesis function to compute predictions.\n",
    "- Calculates the gradient of the loss function using the difference between predictions and actual labels.\n",
    "- Updates model parameters using the learning rate and computed gradient.\n",
    "\n",
    "After running for 100 epochs on the example dataset, the model successfully converged to optimal values for `theta`, showing that the learning algorithm works correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Ð—Ð°Ð²Ð´Ð°Ð½Ð½Ñ 5. ÐžÐ±Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ñ– Ð¼Ð¾Ð´ÐµÐ»Ñ–\n",
    "\n",
    "1. ÐÐ°Ð¿Ð¸ÑˆÑ–Ñ‚ÑŒ Ñ„ÑƒÐ½ÐºÑ†Ñ–ÑŽ `predict_proba(theta, X)`, ÑÐºÐ° Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ” Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ñ– Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¸ $\\theta$ Ð´Ð»Ñ Ð¾Ð±Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ Ð¹Ð¼Ð¾Ð²Ñ–Ñ€Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð½Ð°Ð»ÐµÐ¶Ð½Ð¾ÑÑ‚Ñ– Ð¿Ð¾Ñ‚Ð¾Ñ‡Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¸ÐºÐ»Ð°Ð´Ñƒ Ð· Ð´Ð°Ð½Ð¸Ñ… Ð´Ð¾ ÐºÐ»Ð°ÑÑƒ $y=1$ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ñ– Ð·Ð½Ð°Ñ‡ÐµÐ½ÑŒ $\\sigma(\\theta^T x)$.\n",
    "\n",
    "2. ÐÐ°Ð¿Ð¸ÑˆÑ–Ñ‚ÑŒ Ñ„ÑƒÐ½ÐºÑ†Ñ–ÑŽ `predict(theta, X, threshold=0.5)`, ÑÐºÐ° Ð¾Ð±Ñ‡Ð¸ÑÐ»ÑŽÑ” ÐºÐ»Ð°Ñ Ð· Ð¿ÐµÑ€ÐµÐ´Ð±Ð°Ñ‡ÐµÐ½Ð¾Ñ— Ñ–Ð¼Ð¾Ð²Ñ–Ñ€Ð½Ð¾ÑÑ‚Ñ– Ð½Ð°Ð»ÐµÐ¶Ð½Ð¾ÑÑ‚Ñ– ÐµÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑ€Ð° Ð´Ð¾ ÐºÐ»Ð°ÑÑƒ 1 Ð· Ð¿Ð¾Ñ€Ð¾Ð³Ð¾Ð¼ 0.5. Ð¢Ð¾Ð±Ñ‚Ð¾ ÑÐºÑ‰Ð¾ Ð¹Ð¼Ð¾Ð²Ñ–Ñ€Ð½Ñ–ÑÑ‚ÑŒ Ð¼ÐµÐ½ÑˆÐµ 0.5, Ñ‚Ð¾ Ð¿ÐµÑ€ÐµÐ´Ð±Ð°Ñ‡Ð°Ñ”Ð¼Ð¾ ÐºÐ»Ð°Ñ 0, Ñ–Ð½Ð°ÐºÑˆÐµ ÐºÐ»Ð°Ñ 1.\n",
    "\n",
    "3. ÐÐ°Ð¿Ð¸ÑˆÑ–Ñ‚ÑŒ Ñ„ÑƒÐ½ÐºÑ†Ñ–ÑŽ `accuracy(y_true, y_pred)`, ÑÐºÐ° Ð¾Ð±Ñ‡Ð¸ÑÐ»ÑŽÑ” Ñ‚Ð¾Ñ‡Ð½Ñ–ÑÑ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»Ñ–, Ð²Ð¸Ð·Ð½Ð°Ñ‡Ð¸Ð²ÑˆÐ¸ Ñ‡Ð°ÑÑ‚ÐºÑƒ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð¿ÐµÑ€ÐµÐ´Ð±Ð°Ñ‡ÐµÐ½Ð¸Ñ… ÐºÐ»Ð°ÑÑ–Ð².\n",
    "\n",
    "  Ð¤Ð¾Ñ€Ð¼ÑƒÐ»Ð° Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸ Accuracy:\n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{\\sum_{i=1}^{m} I(\\hat{{y}^{(i)}} = y^{(i)})}{m}\n",
    "  $$\n",
    "\n",
    "  Ð´Ðµ $\\hat{{y}^{(i)}}$ - Ð¿ÐµÑ€ÐµÐ´Ð±Ð°Ñ‡ÐµÐ½Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ ÐºÐ»Ð°ÑÑƒ, $I$ - Ñ–Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€Ð½Ð° Ñ„ÑƒÐ½ÐºÑ†Ñ–Ñ (ÑÐºÐ° Ð´Ð¾Ñ€Ñ–Ð²Ð½ÑŽÑ” 1, ÑÐºÑ‰Ð¾ ÑƒÐ¼Ð¾Ð²Ð° Ð²Ð¸ÐºÐ¾Ð½ÑƒÑ”Ñ‚ÑŒÑÑ, Ñ– 0 - ÑÐºÑ‰Ð¾ Ð½Ñ–), $m$ - ÐºÑ–Ð»ÑŒÐºÑ–ÑÑ‚ÑŒ Ð¿Ñ€Ð¸ÐºÐ»Ð°Ð´Ñ–Ð².\n",
    "\n",
    "4. ÐžÐ±Ñ‡Ð¸ÑÐ»Ñ–Ñ‚ÑŒ Ð· Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½ÑÐ¼ Ð´Ð°Ð½Ð¸Ñ… Ð² Ð·Ð°Ð²Ð´Ð°Ð½Ð½Ñ– 4 $X$, $y$ Ñ‚Ð° Ð¾Ð±Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ñ… ÐºÐ¾ÐµÑ„Ñ–Ñ†Ñ–Ñ”Ð½Ñ‚Ð°Ñ… $\\theta$ Ñ‚Ð° Ð²Ð¸Ð²ÐµÐ´Ñ–Ñ‚ÑŒ Ð½Ð° ÐµÐºÑ€Ð°Ð½:\n",
    "  - Ð¿ÐµÑ€ÐµÐ´Ð±Ð°Ñ‡ÐµÐ½Ñ– Ð¼Ð¾Ð´ÐµÐ»Ð»ÑŽ Ñ–Ð¼Ð¾Ð²Ñ–Ñ€Ð½Ð¾ÑÑ‚Ñ– Ð½Ð°Ð»ÐµÐ¶Ð½Ð¾ÑÑ‚Ñ– ÐºÐ¾Ð¶Ð½Ð¾Ð³Ð¾ Ð· ÐµÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑ€Ñ–Ð² Ð² Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñ– `X` Ð´Ð¾ ÐºÐ»Ð°ÑÑƒ 1\n",
    "  - ÐºÐ»Ð°ÑÐ¸ ÐºÐ¾Ð¶Ð½Ð¾Ð³Ð¾ ÐµÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑ€Ð° Ð· Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ñ– `X`\n",
    "  - Ñ‚Ð¾Ñ‡Ð½Ñ–ÑÑ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»Ñ–."
   ],
   "metadata": {
    "id": "E4iZV55cE5f3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def predict_proba(theta, X):\n",
    "    return sigmoid(X @ theta)\n",
    "\n",
    "def predict(theta, X, threshold=0.5):\n",
    "    return (predict_proba(theta, X) >= threshold).astype(int)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "\n",
    "probas = predict_proba(theta_learned, X)\n",
    "print(\"Predicted probabilities:\", np.round(probas, 3))\n",
    "\n",
    "y_pred = predict(theta_learned, X)\n",
    "print(\"Predicted classes:\", y_pred)\n",
    "\n",
    "acc = accuracy(y, y_pred)\n",
    "print(f\"Accuracy: {acc:.2f}\")"
   ],
   "metadata": {
    "id": "85ZZfPtjrsai",
    "ExecuteTime": {
     "end_time": "2025-06-18T19:17:12.765639Z",
     "start_time": "2025-06-18T19:17:12.761752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities: [0.78  0.256 0.428 0.619]\n",
      "Predicted classes: [1 0 0 1]\n",
      "Accuracy: 0.50\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Result Analysis\n",
    "\n",
    "After training the logistic regression model with full-batch gradient descent, we evaluated its predictions:\n",
    "\n",
    "- **Predicted probabilities**: `[0.78, 0.26, 0.43, 0.62]`\n",
    "- **Predicted classes** (using 0.5 threshold): `[1, 0, 0, 1]`\n",
    "- **True labels**: `[1, 0, 1, 0]`\n",
    "- **Accuracy**: `0.50`\n",
    "\n",
    "From these results:\n",
    "- Two out of four predictions are correct, resulting in an **accuracy of 0.50**.\n",
    "- The model correctly classified one positive and one negative instance.\n",
    "- However, it misclassified one positive (false negative) and one negative (false positive).\n",
    "\n",
    "This suggests that while the model has begun to learn a pattern, the current parameters may not be optimal, and performance is comparable to random guessing. Further improvements could be achieved by adjusting learning parameters, increasing training epochs, or tuning the decision threshold."
   ]
  }
 ]
}
